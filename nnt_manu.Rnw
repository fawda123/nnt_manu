\documentclass[article]{jss}

\usepackage{acronym}
\acrodef{mlp}[mlp]{multilayer perceptron}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Marcus W. Beck\\Oak Ridge Institute for Science and Education\\US Environmental Protection Agency}
\title{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Marcus W. Beck} %% comma-separated
\Plaintitle{NeuralNetTools: Visualization and Analysis Tools for Neural Networks} %% without formatting
\Shorttitle{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks} 

%% an abstract and keywords
\Abstract{
Functions within this package can be used for the interpretation of neural network models created in \proglang{R}, including functions to plot a neural network interpretation diagram, evaluation of variable importance, and a sensitivity analysis of input variables.
}
\Keywords{neural networks, plotnet, sensitivity, variable importance, \proglang{R}}
\Plainkeywords{neural networks, plotnet, sensitivity, variable importance, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Marcus W. Beck\\
  Oak Ridge Institude for Science and Education\\
  US Environmental Protection Agency\\
  National Health and Environmental Effects Research Laboratory\\
  Gulf Ecology Division, 1 Sabine Island Drive\\
  Gulf Breeze, Florida, 32561, USA\\
  E-mail: \email{beck.marcus@epa.gov}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

% knitr options
<<setup,include = F,cache = F>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path = 'figs/', fig.align = 'center', fig.show = 'hold', message = F, echo = F, results = 'asis', dev = 'pdf', dev.args = list(family = 'serif'), fig.pos = '!ht', warning = F)
options(replace.assign = TRUE, digits = 1)
@
% get online bib file
<<echo = FALSE, cache = FALSE>>=
refs <- httr::GET('https://raw.githubusercontent.com/fawda123/refs/master/refs.bib')
refs <- rawToChar(refs$content)
writeLines(refs, con = file('refs.bib'))
@

%% need no \usepackage{Sweave.sty}

\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

The increasing quantity of information and computational capacity to address relevant research questions has contributed to the growth of data science as a legitimate field of study.   Data science is a relatively new paradigm of analysis that focuses on the synthesis of unstructured information from multiple sources to identify patterns or trends `born from the data' \citep{Kelling09}.  A central theme is the focus on data exploration and prediction as compared to specific hypothesis-testing of traditional, domain-specific methods of scientific exploration \citep{Kell03}.  Demand for quantitative toolsets to address challenges in data-rich environments has increased drastically with the continuing advancement of techniques for rapid acquisition of data. Fields of research characterized by high-throughput data have a strong foundation in computationally-intensive methods of analysis (e.g., \citet{Saeys07}).  By contrast, disciplines that have historically been limited by data quantity, such as ecological studies across broad temporal and spatial scales, have shown a need for data intensive approaches given the improved ability to acquire information (e.g., \citet{Swanson15}).  Regardless of the discipline, quantitative approaches that explicitly focus on  inductive reasoning can serve a complementary role to conventional, hypothesis-driven approaches to scientific discovery \citep{Kell03}.  

Statistical methods that have been used to support data exploration for deductive analysis are numerous.  A common theme among many of these methods is the use of machine-learning algorithms where the primary objective is to identify emergent patterns in the data with minimal human intervention.  Neural networks, in particular, are designed to mimic the neuronal structure of the human brain by `learning' inherent data structures through adaptive algorithms.  Although the conceptual model was introduced several decades ago \citep{Maier00}, neural networks continue to maintain a central role in data intensive science.  The most popular form of neural network is the \ac{mlp} trained using the backpropagation algorithm \citep{Rumelhart86}.  This model is typically used to predict the response of one or more variables given co-occurrence of any of a number of explanatory variables.  The hallmark feature of the \ac{mlp} is the characterization of relationships between variabiles using an arbitrary number of parameters (i.e., the hidden layer) that are chosen through an iterative training process with the backpropation algorithm.  Conceptually, the \ac{mlp} is nothing more than a hyper-parameterized non-linear model that can a fit smooth function with almost non-existent residual error for any dataset.

An arbitrarily large number of parameters to fit a neural network provides obvious predictive advantages, but conversely complicates the extraction of critical model information.  Information such as variable importance or model sensitivity are necessary aspects of exploraty data analysis that are not easily obtained from a neural network. As such, a common criticism is that neural networks are `black-boxes' that offer minimal insight into relationships among variables.  \citet{Olden02} provides a rebuttal to this concern by providing methods to extract information from neural networks, most of which were previously available but not commonly used to evaluate neural networks.  For example, \citet{Olden02} describes neural interpretation diagrams for plotting \citep{Ozesmi99}, the Garson algorithm for variable importance \citep{Garson91}, and the Profile method for sensitivity analysis \citep{Lek96}.  These quantitative tools `illuminate the black box' by disaggregating the network parameters as a means to characterize relationships between variables.  In essence, \ac{mlp} neural networks were developed for prediction but methods in \citep{Olden02} leverage these models to describe data signals.  Increasing the accesibility of these diagnostic tools will have value for exploratory analysis in data science.

This article describes the \pkg{NeuralNetTools} package for \proglang{R} that was developed to improve the breadth and quality of information obtained from the \ac{mlp} neural network.  Functions provided by the package are those previously described in \citep{Olden02} but have not been available in an open-source proramming environment.  The reach of the package is all-inclusive such that generic functions were developed using S3 methods for all neural network object classes available in \proglang{R}.  The objecives of this article are to 1) provide an overview of the statistical foundation the \ac{mlp} network, 2) briefly describe similarities and differences between existing neural network packages in \proglang{R}, and 3) describe the theory and application of the primary functions in the \pkg{NeuralNetTools} package.  The package is currently available on CRAN, whereas the development version is maintained as a GitHub repository.  

\section[Theoretical foundation]{Theoretical foundation and existing R packages}

Packages available in R to create neural networks (similarities, differences)

hyper-dimensional

Formulaically \citep{Ripley96}
$y_k = f_k \left(\sum\limits_{j=1}^k w_{jk}f_j \left( \sum\limits_{i=1}^j w_{ij}x_i\right) \right) $

\section[Package structure]{Package structure}



\subsection{Visualizing neural networks}

The number of existing functions in \pkg{R} to view neural networks is minimal.  Such tools have practical use for visualizing network architecture and connections between layers that mediate variable importance. To our knowledge, only the \pkg{neuralnet} package provides a method for plotting neural networks created with the \code{neuralnet} function.  Although useful for viewing the basic structure, the output is visually minimal and does not include options for customization (verify).

This function plots a neural network as a neural interpretation diagram as in \citet{Ozesmi99}. Options to plot without color-coding or shading of weights are also provided. The default settings plot positive weights between layers as black lines and negative weights as grey lines. Line thickness is in proportion to relative magnitude of each weight. The first layer includes only input variables with nodes labelled arbitrarily as I1 through In for n input variables. One through many hidden layers are plotted with each node in each layer labelled as H1 through Hn. The output layer is plotted last with nodes labeled as O1 through On. Bias nodes connected to the hidden and output layers are also shown. Neural networks created using \code{mlp} do not show bias layers.

A primary network and a skip layer network can be plotted for nnet models with a skip layer connection. The default is to plot the primary network, whereas the skip layer network can be viewed with \code{skip = TRUE}. If \code{nid = TRUE}, the line widths for both the primary and skip layer plots are relative to all weights. Viewing both plots is recommended to see which network has larger relative weights. Plotting a network with only a skip layer (i.e., no hidden layer, \code{size = 0}) will include bias connections to the output layer, whereas these are not included in the plot of the skip layer if size is greater than zero.

Pruned networks in RSNNS.

\subsection{Evaluating variable importance}

The \code{garson} function uses Garson's algorithm to evaluate relative variable importance. This function identifies the relative importance of explanatory variables for a single response variable by deconstructing the model weights. The importance of each variable can be determined by identifying all weighted connections between the layers in the network. That is, all weights connecting the specific input node that pass through the hidden layer to the response variable are identified. This is repeated for all other explanatory variables until a list of all weights that are specific to each input variable is obtained. The connections are tallied for each input node and scaled relative to all other inputs. A single value is obtained for each explanatory variable that describes the relationship with the response variable in the model. The results indicate relative importance as the absolute magnitude from zero to one. The function cannot be used to evaluate the direction of the response. Only neural networks with one hidden layer and one output node can be evaluated.

The \code{olden} function is an alternative and more flexible approach to evaluate variable importance. The function calculates importance as the product of the raw input-hidden and hidden-output connection weights between each input and output neuron and sums the product across all hidden neurons. An advantage of this approach is the relative contributions of each connection weight are maintained in terms of both magnitude and sign as compared to Garson's algorithm which only considers the absolute magnitude. For example, connection weights that change sign (e.g., positive to negative) between the input-hidden to hidden-output layers would have a cancelling effect whereas Garson's algorithm may provide misleading results based on the absolute magnitude. An additional advantage is that Olden's algorithm is capable of evaluating neural networks with multiple hidden layers and response variables. The importance values assigned to each variable are in units that are based directly on the summed product of the connection weights. The actual values should only be interpreted based on relative sign and magnitude between explanatory variables. Comparisons between different models should not be made.

Issues with different indications of variable importance as a model is refit...

\subsection{Sensitivity analysis}

The Lek profile method is described briefly in \citet{Lek96} and in more detail in \citet{Gevrey03}. The profile method is fairly generic and can be extended to any statistical model in R with a predict method. However, it is one of few methods used to evaluate sensitivity in neural networks.

The profile method can be used to evaluate the effect of explanatory variables by returning a plot of the predicted response across the range of values for each separate variable. The original profile method evaluated the effects of each variable while holding the remaining expalanatory variables at different quantiles (e.g., minimum, 20th percentile, maximum). This is implemented in in the function by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (or data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for each explanatory variable to obtain all response curves. Values passed to \code{split_vals} must range from zero to one to define the quantiles for holding unevaluated explanatory variables.

An alternative implementation of the profile method is to group the unevaluated explanatory variables using groupings defined by the statistical properties of the data. Covariance among predictors may present unlikely scenarios if holding all unevaluated variables at the same level. To address this issue, the function provides an option to hold unevalutaed variable at mean values defined by natural clusters in the data. kmeans clustering is used on the input data.frame of explanatory variables if the argument passed to \code{split_vals} is an integer value greater than one. The centers of the clusters are then used as constant values for the unevaluated variables. An arbitrary grouping scheme can also be passed to \code{split_vals} as a data.frame where the user can specify exact values for holding each value constant (see the examples).  Examples in \citet{Beck14a} show this...

For all plots, the legend with the 'splits' label indicates the colors that correspond to each group. The groups describe the values at which unevaluated explanatory variables were held constant, either as specific quantiles, group assignments based on clustering, or in the arbitrary grouping defined by the user. The constant values of each explanatory variable for each split can be viewed as a barplot by using \code{split_show = TRUE}.

Note that there is no predict method for neuralnet objects from the nn package. The lekprofile method for nn objects uses the nnet package to recreate the input model, which is then used for the sensitivity predictions. This approach only works for networks with one hidden layer.

\section[Future development]{Future development}


\section[Conclusions]{Conclusions}

A cautionary note about the `optimal network' and reproducibility of results.  

\section[Acknowledgments]{Acknowledgments}

\bibliographystyle{jss}
\bibliography{refs}

\end{document}
