\documentclass[article,shortnames]{jss}

\usepackage{acronym}
\usepackage{cleveref}
\usepackage{subfig}
\usepackage{framed}

\acrodef{CRAN}[CRAN]{Comprehensive \proglang{R} Archive Network}
\acrodef{mlp}[MLP]{multilayer perceptron}
\acrodef{nid}[NID]{neural interpretation diagram}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Marcus W. Beck\\US Environmental Protection Agency}
\title{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Marcus W. Beck} %% comma-separated
\Plaintitle{NeuralNetTools: Visualization and Analysis Tools for Neural Networks} %% without formatting
\Shorttitle{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks} 

%% an abstract and keywords
\Abstract{
Supervised neural networks have been applied as a machine learning technique to identify and predict emergent patterns in unstructured datasets.  A common criticism of the method is the inability to characterize relationships among variables from a fitted model. Although several methods have been proposed to `illuminate the black box', they have not been made available in an open-source programming environment.  This article describes the \pkg{NeuralNetTools} package that can be used for the interpretation of supervised neural network models created in \proglang{R}.  Functions in the package can be used to visualize a model using a neural network interpretation diagram, evaluate variable importance by disaggregating the model weights, and perform a sensitivity analysis of the response variables to changes in the input variables.  Methods are provided for objects from many of the common neural network packages in \proglang{R}, including \pkg{caret}, \pkg{neuralnet}, \pkg{nnet}, and \pkg{RSNNS}.  The article provides a brief overview of the theoretical foundation of neural networks, a description of the package structure and functions, and an applied example to provide a context for model development with \pkg{NeuralNetTools}.  Overall, the package provides a toolset for neural networks that complements existing quantitative techniques for data-intensive exploration.      
}
\Keywords{neural networks, plotnet, sensitivity, variable importance, \proglang{R}}
\Plainkeywords{neural networks, plotnet, sensitivity, variable importance, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Marcus W. Beck\\
  US Environmental Protection Agency\\
  National Health and Environmental Effects Research Laboratory\\
  Gulf Ecology Division, 1 Sabine Island Drive\\
  Gulf Breeze, Florida, 32561, USA\\
  Telephone: +1/850/934-2480\\
  E-mail: \email{beck.marcus@epa.gov}
}

% options and libs
% get online bib file

%% need no \usepackage{Sweave.sty}

\begin{document}


%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

Data-intensive analysis is a relatively new research approach that focuses on the synthesis of unstructured information to identify patterns or trends `born from the data' \citep{Bell09,Kelling09,Michener12}.  A central theme is the focus on data exploration and prediction as compared to hypothesis-testing using domain-specific methods for scientific exploration \citep{Kell03}.  Demand for quantitative toolsets to address challenges in data-rich environments has increased drastically with the advancement of techniques for rapid acquisition of data. Fields of research characterized by high-throughput data (e.g., bioinformatics, \citet{Saeys07}) have a strong foundation in computationally-intensive methods of analysis, whereas disciplines that have historically been limited by data quantity (e.g., field ecology, \citet{Swanson15}) have also realized the importance of quantitative toolsets given the development of novel techniques to acquire information.  Quantitative methods that facilitate inductive reasoning can serve a complementary role to conventional, hypothesis-driven approaches to scientific discovery \citep{Kell03}.  

Statistical methods that have been used to support data exploration are numerous \citep{Jain00,Recknagel06,Zuur10}.  A common theme among data intensive methods is the use of machine-learning algorithms where the primary objective is to identify emergent patterns with minimal human intervention.  Neural networks, in particular, are designed to mimic the neuronal structure of the human brain by `learning' inherent data structures through adaptive algorithms \citep{Rumelhart86,Ripley96}.  Although the conceptual model was introduced several decades ago \citep{McCulloch43}, neural networks have had a central role in emerging fields focused on data exploration.  The most popular form of neural network is the feed-forward \ac{mlp} trained using the backpropagation algorithm \citep{Rumelhart86}.  This model is typically used to predict the response of one or more variables given one to many explanatory variables.  The hallmark feature of the \ac{mlp} is the characterization of relationships using an arbitrary number of parameters (i.e., the hidden layer) that are chosen through iterative training with the backpropagation algorithm.  Conceptually, the \ac{mlp} is a hyper-parameterized non-linear model that can fit a smooth function to any dataset with minimal residual error \citep{Hornik91}.

An arbitrarily large number of parameters to fit a neural network provides obvious predictive advantages, but conversely complicates the extraction of model information.  Diagnostic information such as variable importance or model sensitivity are necessary aspects of exploratory data analysis that are not easily obtained from a neural network. As such, a common criticism is that neural networks are `black boxes' that offer minimal insight into relationships among variables \citep[e.g.,][]{Paruelo97}.  \citet{Olden02} provide a rebuttal to this concern by describing methods to extract information from neural networks.  Many of these methods were previously described but not commonly used.  For example, \citet{Olden02} describe  the \ac{nid} for plotting \citep{Ozesmi99}, the Garson algorithm for variable importance \citep{Garson91}, and the Profile method for sensitivity analysis \citep{Lek96}.  These quantitative tools `illuminate the black box' by disaggregating the network parameters to characterize relationships between variables that are described by the model.  Although \ac{mlp} neural networks were developed for prediction, methods described in \citet{Olden02} leverage these models to describe data signals.  Increasing the accesibility of these diagnostic tools will have value for exploratory data analysis and may also inform causal inference.

This article describes the \pkg{NeuralNetTools} package for \proglang{R} that was developed to improve the breadth and quality of information obtained from the \ac{mlp} neural network.  Functions provided by the package are those described in \citet{Olden02} but have not been previously available in an open-source programming environment.  The reach of the package is extensive in that generic functions were developed for model objects from the most popular neural network packages available in \proglang{R}.  The objectives of this article are to 1) provide an overview of the statistical foundation of the \ac{mlp} network, 2) describe the theory and application of the main functions in the \pkg{NeuralNetTools} package, and 3) provide an applied example using neural networks and \pkg{NeuralNetTools} in data exploration.  The package is currently available on the \ac{CRAN}, whereas the development version is maintained as a GitHub repository.  

\section[Theoretical foundation]{Theoretical foundation and existing R packages}

The typical \ac{mlp} network is composed of multiple layers that define the transfer of information between input and response layers.  Information travels in one direction where a set of values for variables in the input layer propagates through one or more hidden layers to the final layer of the response variables. Hidden layers between the input and response layers are key components of a neural network that mediate the transfer of information.  Just as the input and response layers are composed of variables or nodes, each hidden layer is composed of nodes with weighted connections that define the strength of information flow between layers.  Bias layers connected to hidden and response layers may also be used that are analagous to intercept terms in a standard regression model.

Training a neural network model requires identifying the optimal weights that define the connections between the model layers.  The optimal weights are those that minimize prediction error for a test dataset that is independent of the training dataset.  Training is commonly achieved using the backpropagation algorithm described in \citep{Rumelhart86}.  This algorithm identifies the optimal weighting scheme through an iterative process where weights are gradually changed through a forward- and backward-propagation process \citep{Rumelhart86,Lek00}.  The algorithm begins by assigning an arbitrary weighting scheme to the connections in the network, followed by estimating the output in the response variable through the forward-propagation of information through the network, and finally calculating the difference between the predicted and actual value of the response.  The weights are then changed through a back-propagation step that begins by changing weights in the output layer and then the remaining hidden layers.  The process is repeated until the chosen error function is minimized, as in standard model-fitting techniques for regression \citep{Cheng94}.  Formulaically, a fitted \ac{mlp} neural network can be represented as \citep{Bishop95,Venables02}:
\begin{equation}
y_k = f_o \left(\sum\limits_{h} w_{hk}f_h \left( \sum\limits_{i} w_{ih}x_i\right) \right)
\end{equation}
where the estimated value $k$ of the response variable $y$ is a summation of the products between the respective weights $w$ for $i$ input variables $x$ and $h$ hidden nodes, mediated by the activation functions $f_h$ and $f_o$ for each hidden and output node.


Methods in \pkg{NeuralNetTools} were written for several \proglang{R} packages that can be used to create \ac{mlp} neural networks: \pkg{neuralnet} \citep{Fritsch12}, \pkg{nnet} \citep{Venables02}, and \pkg{RSNNS} \citep{Bergmeir12}. Limited methods were also developed for neural network objects created with the \code{train} function from the \pkg{caret} package \citep{Kuhn15}.  Additional \proglang{R} packages that can create \ac{mlp} neural networks include \pkg{AMORE} that implements the `TAO-robust back-propagation algorithm' for model fitting \citep{Castejon14}, \pkg{FCNN4R} as an \proglang{R} interface to the \pkg{FCNN} \proglang{C++} library \citep{Klima15}, \pkg{monmlp} for networks with partial monotonicity constraints \citep{Cannon15}, and \pkg{qrnn} for quantile regression neural networks \citep{Cannon11}.  At the time of writing, the \ac{CRAN} download logs \citep{Csardi15} showed that the \proglang{R} packages with methods in \pkg{NeuralNetTools} included 94\% of all downloads for the available \ac{mlp} packages, with \pkg{nnet} accounting for over 80\%.  As such, methods have not been included in \pkg{NeuralNetTools} for the remaining packages, although further development of \pkg{NeuralNetTools} could include additional methods based on popularity.  Methods for each function are currently available for  \code{mlp} (\pkg{RSNNS}), \code{nn} (\pkg{neuralnet}), \code{nnet} (\pkg{nnet}), and \code{train} (\pkg{caret}, only if the object also inherits from the \code{nnet} class.  Additional \code{default} or \code{numeric} methods are available for some of the generic functions.

\section[Package structure]{Package structure}

The stable release of \pkg{NeuralNetTools} can be installed from \ac{CRAN} and loaded as follows:

\begin{Schunk}
\begin{Sinput}
R> install.packages('NeuralNetTools')
R> library(NeuralNetTools)
\end{Sinput}
\end{Schunk}

\pkg{NeuralNetTools} includes four main functions that were developed following similar techniques in \citet{Olden02} and references therein.  Functions include \code{plotnet} to plot a neural network interpretation diagram, \code{garson} and \code{olden} functions to evaluate variable importance, and \code{lekprofile} for a sensitivity analysis of neural network response to input variables.  Most of the functions require the extraction of model weights in a common format for the neural network object classes in \proglang{R}.  The \code{neuralweights} function can be used to retrieve model weights for any of the model classes described above.  A two-element \code{list} is returned with the first element describing the structure of the network (number of nodes in the input, hidden, and output layers) and the second element as a named list of model weights.  The function is used internally within the main functions but may be useful for comparing networks of different classes.

A common approach for data preprocessing is to normalize the input variables and to standardize the response variables \citep{Lek00,Olden02}.  A sample dataset that follows this format is included with \pkg{NeuralNetTools}.  The \code{neuraldat} dataset is a simple \code{data.frame} with 2000 rows of observations and five columns for two response variables (\code{Y1} and \code{Y2}) and three input variables (\code{X1}, \code{X2}, and \code{X3}).  The input variables are random observations from a standard normal distribution and the response variables are linear combinations of the input variables with additional random components.  The response variables are also standardized from zero to one.  Additional datasets can be preprocessed to this common format using the \code{scale} function from \pkg{base} and the \code{rescale} function from \pkg{scales} for the input and response variables, respectively.  The examples below use three models created from the \code{neuraldat} dataset and include \code{mlp} (\pkg{RSNNS}), \code{nn} (\pkg{RSNNS}), and \code{nnet} (\pkg{nnet}) objects.

\begin{Schunk}
\begin{Sinput}
R> set.seed(123)
R> library(RSNNS)
R> x <- neuraldat[, c('X1', 'X2', 'X3')]
R> y <- neuraldat[, 'Y1']
R> mod1 <- mlp(x, y, size = 5)
R> library(neuralnet)
R> mod2 <- neuralnet(Y1 ~ X1 + X2 + X3, data = neuraldat, hidden = 5)
R> library(nnet)
R> mod3 <- nnet(Y1 ~ X1 + X2 + X3, data = neuraldat, size = 5)
\end{Sinput}
\end{Schunk}

\subsection{Visualizing neural networks}

Existing plot functions in \proglang{R} to view neural networks are minimal.  Such tools have practical use for visualizing network architecture and connections between layers that mediate variable importance. To our knowledge, only the \pkg{neuralnet} and \pkg{FCNN4R} packages provide plotting methods for \ac{mlp} networks in \proglang{R}.  Although useful for viewing the basic structure, the output is minimal and does not include extensive options for customization.

Tbe \code{plotnet} function in \pkg{NeuralNetTools} plots a \acl{nid} \citep[\acs{nid},][]{Ozesmi99} and includes several options to customize aesthetics. A \ac{nid} is a modification to the standard conceptual illustration of the \ac{mlp} network that changes the thickness and color of the weight connections based on magnitude and sign, respectively.  Positive weights between layers are shown as black lines and negative weights as grey lines. Line thickness is in proportion to absolute magnitude of each weight (\cref{fig:plotnet}).

\begin{figure}[!ht]
\subfloat[][]{
\includegraphics[width=7.5cm,page=1]{figs/plotnet.pdf}
\label{fig:plotnet1}
}
\subfloat[][]{
\includegraphics[width=7.5cm, page=2]{figs/plotnet.pdf}
\label{fig:plotnet2}
}
\caption{Examples from the \code{plotnet} function showing neural networks as a standard graphic (\ref{fig:plotnet1}) and using the \acl{nid} (\ref{fig:plotnet2}).  Labels outside of the nodes represent variable names and labels within the nodes indicate the layer and node (I: input, H: hidden, O: output, B: bias).}
\label{fig:plotnet}
\end{figure}

A primary and skip layer network can also be plotted for \code{nnet} models with a skip layer connection (\cref{fig:plotnet_skip}). Models with skip layers include additional connections from the input to output layers that bypass the hidden layer \citep{Ripley96}.  The default behavior of \code{plotnet} is to plot the primary network, whereas the skip layer can be viewed separately with \code{skip = TRUE}. If \code{nid = TRUE}, the line widths for both the primary and skip layer plots are relative to all weights. Plotting a network with only a skip layer (i.e., no hidden layer, \code{size = 0} in \code{nnet}) will include bias connections to the output layer, whereas these are included only in the primary plot if size is greater than zero.

\begin{figure}[!ht]
\subfloat[][]{
\includegraphics[width=7.5cm,page=1]{figs/plotnet_skip.pdf}
\label{fig:plotnet_skip1}
}
\subfloat[][]{
\includegraphics[width=7.5cm, page=2]{figs/plotnet_skip.pdf}
\label{fig:plotnet_skip2}
}
\caption{Examples from the \code{plotnet} function showing a neural network with a separate skip layer between the input and output layers.  The skip layer (\ref{fig:plotnet_skip1}) and primary neural network (\ref{fig:plotnet_skip2}) can be viewed separately with \code{plotnet} by using \code{skip = TRUE} or \code{skip = FALSE}.}
\label{fig:plotnet_skip}
\end{figure}

The \pkg{RSNNS} package provides algorithms to prune connections or nodes in a neural network \citep{Bergmeir12}.  This approach can remove connection weights between layers or input nodes that do not contribute to the predictive performance of the network.  Pruning has additional benefits of an increase in power by increasing the degrees of freedom or the identification of input nodes with low importance.  Algorithms in \pkg{RSNNS} for weight pruning include magnitude based pruning, Optimal Brain Damage, and Optimal Brain Surgeon, whereas algorithms for node pruning include skeletonization and the non-contributing units method \citep{Zell98}.  The \code{plotnet} function can plot a pruned neural network, with options to omit or display the pruned connections (\cref{fig:plotprune}).  

\begin{Schunk}
\begin{Sinput}
R> pruneFuncParams <- list(max_pr_error_increase = 10.0, pr_accepted_error = 1.0,
+   no_of_pr_retrain_cycles = 1000, min_error_to_stop = 0.01, 
+   init_matrix_value = 1e-6, input_pruning = TRUE, hidden_pruning = TRUE)
R> mod <- mlp(x, y, size = 5, pruneFunc = "OptimalBrainSurgeon",
+  pruneFuncParams = pruneFuncParams)
R> plotnet(mod, rel_rsc = c(3, 8))
R> plotnet(mod, prune_col = 'lightblue', rel_rsc = c(3, 8))
\end{Sinput}
\end{Schunk}
\begin{figure}[!ht]
\subfloat[][]{
\includegraphics[width=0.48\textwidth, page=1]{figs/plotprune.pdf}
\label{fig:plotprune1}
}
\subfloat[][]{
\includegraphics[width=0.48\textwidth, page=2]{figs/plotprune.pdf}
\label{fig:plotprune2}
}
\caption{A pruned neural network from \pkg{RSNNS} \citep{Bergmeir12} using the `Optimal Brain Surgeon' algorithm described in \citet{Zell98}.  The default plotting behavior of \code{plotnet} is to omit pruned connections (\ref{fig:plotprune1}), whereas they can be viewed as dashed lines by including the \code{prune\_col} argument (\ref{fig:plotprune2}).}
\label{fig:plotprune}
\end{figure}

\subsection{Evaluating variable importance}

The primary benefits of visualizing a \ac{nid} with \code{plotnet} is the ability to evaluate network architecture and the variation in connections between the layers.  Although useful as a general tool, the \ac{nid} can be difficult to interpret given the amount of weighted connections in most networks.  Alternative methods to quantitatively describe a neural network deconstruct the model weights to determine variable importance, whereas similar information can only be qualitatively inferred from \code{plotnet}.  Two algorithms for evaluating variable importance are available in \pkg{NeuralNetTools}: Garson's algorithm for relative importance \citep{Garson91,Goh95} and Olden's conection weights algorithm \citep{Olden04}.

Garson's algorithm was originally described by \citet{Garson91} and further modified by \citet{Goh95}.  The \code{garson} function is an implementation of the method described in the appendix of \citet{Goh95} that identifies the relative importance of each variable as an absolute magnitude. For each input node, all weights connecting an input through the hidden layer to the response variable are identified to return a list of all weights specific to each input variable. A summed product of the connections for each input node are then scaled relative to all other inputs. A value for each input node indicates relative importance as the absolute magnitude from zero to one. The method is limited in that the direction of the response cannot be determined and only neural networks with one hidden layer and one output node can be evaluated.

The \code{olden} function is a more flexible approach to evaluate variable importance using the connection weights algorithm \citep{Olden04}. This method calculates importance as the summed product of the raw input-hidden and hidden-output connection weights between each input and output node. An advantage is the relative contributions of each connection weight are maintained in both magnitude and sign. For example, connection weights that change sign (e.g., positive to negative) between the input-hidden to hidden-output layers would have a cancelling effect, whereas Garson's algorithm may provide different results based on the absolute magnitude. An additional advantage is that Olden's algorithm can evaluate neural networks with multiple hidden layers and response variables. The importance values assigned to each variable are also in units based on the summed product of the connection weights, whereas \code{garson} returns importance scaled from 0--1.

Both functions have similar implementations and require only a model object as input.  The default output is a \pkg{ggplot2} bar plot \citep[i.e., \code{geom\_bar},][]{Wickham09} that shows the relative importance of each input variable in the model (\cref{fig:plotimp}).  The plot aesthetics are based on internal code that can be changed using conventional syntax for \pkg{ggplot2} applied to the output object.  The importance values can also be returned as a \code{data.frame} if \code{bar\_plot = FALSE}.  Variable importance shown in \Cref{fig:plotimp} was estimated for each model:
\begin{Schunk}
\begin{Sinput}
R> garson(mod1)
R> olden(mod1)
R> garson(mod2)
R> olden(mod2)
R> garson(mod3)
R> olden(mod3)
\end{Sinput}
\end{Schunk}
\begin{figure}[!h]
\centering
\subfloat[][Model 1 (\code{mlp}), \code{garson}]{
\includegraphics[width = 0.47\textwidth, page = 1]{figs/plotimp.pdf}
\label{fig:plotimp1}
}
\subfloat[][Model 1 (\code{mlp}), \code{olden}]{
\includegraphics[width = 0.47\textwidth, page = 2]{figs/plotimp.pdf}
\label{fig:plotimp2}
}

\subfloat[][Model 2 (\code{nn}), \code{garson}]{
\includegraphics[width = 0.47\textwidth, page = 3]{figs/plotimp.pdf}
\label{fig:plotimp3}
}
\subfloat[][Model 2 (\code{nn}), \code{olden}]{
\includegraphics[width = 0.47\textwidth, page = 4]{figs/plotimp.pdf}
\label{fig:plotimp4}
}

\subfloat[][Model 3 (\code{nnet}), \code{garson}]{
\includegraphics[width = 0.47\textwidth, page = 5]{figs/plotimp.pdf}
\label{fig:plotimp5}
}
\subfloat[][Model 3 (\code{nnet}), \code{olden}]{
\includegraphics[width = 0.47\textwidth, page = 6]{figs/plotimp.pdf}
\label{fig:plotimp6}
}
\caption{Variable importance for three models using Garson's algorithm for relative importance (\code{garson}, \cref{fig:plotimp1,fig:plotimp3,fig:plotimp5}, \citealt{Garson91,Goh95}) and Olden's conection weights algorithm (\code{olden}, \cref{fig:plotimp2,fig:plotimp4,fig:plotimp6}, \citealt{Olden04}).  Garson's algorithm shows importance as absolute values from 0--1, whereas Olden's algorithm preserves sign and magnitude.  Importance values for Olden's algorithm are from the summed product of model weights and are not rescaled.}
\label{fig:plotimp}
\end{figure}

\subsection{Sensitivity analysis}

An alternative approach to evaluate variable interactions in a neural network is the Lek profile method, described briefly in \citet{Lek96} and in more detail in \citet{Gevrey03}. The profile method differs fundamentally from the variable importance algorithms by evaluating the behavior of response variables across different values of the input variables. The method is generic and can be extended to any statistical model in \pkg{R} with a \code{predict} method. However, it is one of few methods used to evaluate sensitivity in neural networks.  

The \code{lekprofile} function evaluates the effects of input variables by returning a plot of model predictions across the range of values for each variable.  The remaining explanatory variables are held constant when evaluating the effects of each input variable.  The \code{lekprofile} function provides two options for setting constant values of unevaluated explanatory variables.  The first option follows the original profile method by holding unevaluated variables at different quantiles (e.g., minimum, 20th percentile, maximum, \cref{fig:plotlek_sens1,fig:plotlek_bars1}). This is implemented by creating a matrix where the number of rows is the number of observations in the original dataset and the number of columns is the number of explanatory variables. All explanatory variables are held at a chosen constant value (e.g., median), while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix is then used to predict values of the response variable from a fitted model object. This is repeated for each explanatory variable to obtain all response curves.  Constant values are set in \code{lekprofile} by passing one or more values in the range 0--1 to the \code{group\_vals} argument.  The default holds variables at the minimum, 20\textsuperscript{th}, 40\textsuperscript{th}, 60\textsuperscript{th}, 80\textsuperscript{th}, and maximum percentiles (i.e., \code{group\_vals = c(0, 0.2, 0.4, 0.6, 0.8, 1)}).  

A second implementation of \code{lekprofile} is to group the unevaluated explanatory variables by natural groupings defined by the data. Covariance among predictors may present unlikely scenarios if all unevaluated variables are held at the same level (e.g., high values for one variable may be unlikely with high values for a second variable). The second option holds unevaluated variables at mean values defined by natural clusters in the data (\cref{fig:plotlek_sens2,fig:plotlek_bars2}). Clusters are identified using {\it k}-means clustering (\code{kmeans} from base \pkg{stats}, \citealt{Hartigan79}) of the input variables if the argument passed to \code{group_vals} is an integer value greater than one. The centers (means) of the clusters are then used as constant values for the unevaluated variables.  \citet{Beck14a} provide an example of the clustering method for \code{lekprofile} by evaluating response of a lake health index to different explanatory variables.  Lake clusters were identified given covariance among variables, such that holding explanatory variables at values defined by clusters created more ecologically valid response curves.  Both methods return similar plots, with additional options to visualize the groupings for unevaluated explanatory variables (\cref{fig:plotlek_bars}).  For the latter case, \code{group\_show = TRUE} will return a stacked bar plot for each group with heights within each bar proportional to the constant values.  Sensitivity profiles were created using the standard approach based on quantiles and using the alternative clustering method (\cref{fig:plotlek_sens}), including bar plots of the relative values for unevaluated explanatory variables (\cref{fig:plotlek_bars}).
\begin{Schunk}
\begin{Sinput}
R> lekprofile(mod3)
R> lekprofile(mod3, group_show = TRUE)
R> lekprofile(mod3, group_vals = 6)
R> lekprofile(mod3, group_vals = 6, group_show = TRUE)
\end{Sinput}
\end{Schunk}
\begin{figure}[!ht]
\centering
\subfloat[][Response with quantiles]{
\includegraphics[width = 0.9\textwidth, page = 1]{figs/plotlek_sens.pdf}
\label{fig:plotlek_sens1}
}

\subfloat[][Response with clusters]{
\includegraphics[width = 0.9\textwidth, page = 2]{figs/plotlek_sens.pdf}
\label{fig:plotlek_sens2}
}
\caption{Sensitivity analysis of a neural network using the Lek profile method to evaluate the effects of explanatory variables.  \Cref{fig:plotlek_sens1} groups unevaluated explanatory variables at quantiles (minimum, 20\textsuperscript{th}, 40\textsuperscript{th}, 60\textsuperscript{th}, 80\textsuperscript{th}, and maximum percentiles) and \cref{fig:plotlek_sens2} groups by cluster means (six groups).  Values at which explanatory variables are held constant for each group are shown in \cref{fig:plotlek_bars1,fig:plotlek_bars2}.}
\label{fig:plotlek_sens}
\end{figure}

%%%%
\begin{figure}
\centering
\subfloat[][Quantile groupings]{
\includegraphics[width = 0.9\textwidth, page = 1]{figs/plotlek_bars.pdf}
\label{fig:plotlek_bars1}
}

\subfloat[][Cluster groupings]{
\includegraphics[width = 0.9\textwidth, page = 2]{figs/plotlek_bars.pdf}
\label{fig:plotlek_bars2}
}
\caption{Bar plots for values of unevaluated explanatory variables in each group in \cref{fig:plotlek_sens1,fig:plotlek_sens2}.  \Cref{fig:plotlek_bars1} shows default quantile groupings set at the minimum, 20\textsuperscript{th}, 40\textsuperscript{th}, 60\textsuperscript{th}, 80\textsuperscript{th}, and maximum percentiles.  For example, variables are held at negative values for group 1 (i.e., stacked bars with negative heights) for the minimum value, whereas group 6 holds variables at their maximum (largest positive heights). \Cref{fig:plotlek_bars2} shows the cluster centers for each variable in each group.  Groups in \cref{fig:plotlek_bars2} are random because the input variables are from a standard normal distribution.}
\label{fig:plotlek_bars}
\end{figure}

\section[Applied example]{Applied example}

Although \pkg{NeuralnetTools} provides several methods to extract information from a fitted neural network, it does not provide explicit guidance for developing the initial model.  A potentially more challenging aspect of using \ac{mlp} neural networks is understanding the effects of network architecture on model performance, appropriate use of training and validation datasets, and implications for the bias-variance tradeoff with model over- or under-fitting \citep{Maier00}.  A detailed discussion of these issues is beyond the scope of this paper, although an example application is presented below to emphasize the importance of these considerations.  The models presented above, including the \code{neuraldat} dataset, are contrived examples to illustrate use of the \pkg{NeuralNetTools} package and they do not demonstrate a comprehensive or practical application of model development.  In general, the following should be considered during initial development \citep{Ripley96, Lek00, Maier00}:
\begin{itemize}
\item Initial data pre-processing to normalize inputs, standardize response, and assess influence of outliers,
\item Network architecture including number of hidden layers, number of nodes in each hidden layer, inclusion of bias or skip layers, and pruning weights or inputs,
\item Separating data into training and test datasets, e.g., 2:1, 3:1, leave-one-out, etc., 
\item Initial starting weights for the back-propagation algorithm, and
\item Criteria for stopping model training, e.g., error convergance tolerance, maximum number of iterations, minimum error on test dataset, etc.
\end{itemize}

A dataset from \pkg{nycflights13} \citep{Wickham14b} is used to demonstrate effects of training conditions on model conclusions (get analysis code \href{https://raw.githubusercontent.com/fawda123/nnt_manu/master/flightimp.R}{here}).  This dataset provides information on all flights departing New York City (i.e., JFK, LGA, or EWR) in 2013.  The example uses all flights from the UA carrier in the month of December to identify variables that potentially influence arrival delays (\code{arr\_delay}, minutes) at the destination airport.  Factors potentially related to delays were selected from the dataset and included departure delay (\code{dep\_delay}, minutes), departure time (\code{dep\_time}, hours, minutes), arrival time (\code{arr\_time}, hours, minutes), travel time between desinations (\code{air\_time}, minutes), and distance flown (\code{distance}, miles).  All explanatory variables were normalized and the response variable was standardized to 0--1.

The effects of network architecture and initial starting weights on estimates of variable importance were evaluated by creating many models and assessing the uncertainty of the importance.  Models with one, five, or ten hidden nodes and 100 separate models for each node level were created.  Each model had a random set of starting weights for the first training iteration.  Importance estimates using \code{olden} were saved for each model and combined in a single plot to show overall variable importance as the median and 5\textsuperscript{th}/95\textsuperscript{th} percentiles from the 100 models for each node level (see analysis \href{https://raw.githubusercontent.com/fawda123/nnt_manu/master/flightimp.R}{code}). 

Several conclusions can be made from \cref{fig:flightimp}.  First, delays in arrival time are negatively related to distance and positively related to departure delays and travel air.  That is, flights arrived later than their scheduled time if flight time was long or if their departure was delayed, whereas flights arrived earlier than scheduled for longer distances.  No conclusions can be made for the other variables because the bounds of uncertainty included zero.  Second, the range of importance estimates varied between the models (i.e., one node  varied +/- 1 and the others +/- 3).  This suggests that the relative importance estimates only have relevance within each model, whereas only the rankings (e.g., least, most important) can be compared between models.  Third and most important, the level of uncertainty for specific variables can be large between model fits for the same architecture.  This suggests that a single model can provide misleading information such that several models may be required to improve the certainty of conclusions.  Additional considerations described above (e.g., criteria for stopping training, use of training and test datasets) can also affect the interpration of model information and should be equally considered during model development.
\begin{figure}[!h]
\centering
\subfloat[][Networks with one node]{
\includegraphics[width = 0.95\textwidth, page = 1]{figs/flightimp.pdf}
\label{fig:flightimp1}
}

\subfloat[][Networks with five nodes]{
\includegraphics[width = 0.95\textwidth, page = 2]{figs/flightimp.pdf}
\label{fig:flightimp2}
}

\subfloat[][Networks with ten nodes]{
\includegraphics[width = 0.95\textwidth, page = 3]{figs/flightimp.pdf}
\label{fig:flightimp3}
}
\caption{Uncertainty in variable importance estimates for three neural networks to evaluate factors related to arrival delays for flights departing New York City.  Three model types with one, five, and ten nodes were evaluated with 100 models with different starting weights for each type.}
\label{fig:flightimp}
\end{figure}

\section[Conclusions]{Conclusions}

The \pkg{NeuralNetTools} package provides a simple approach to improve the quality of information obtained from a feed-forward \ac{mlp} neural network.  Functions can be used to visualize a neural network using a \acl{nid} (\code{plotnet}), evaluate variable importance (\code{garson}, \code{olden}), and conduct a sensitivity analysis (\code{lekprofile}).  Methods are available for the most used \ac{CRAN} packages that can create neural networks (\pkg{caret}, \pkg{neuralnet}, \pkg{nnet}, \pkg{RSNNS}), whereas additional methods could be added based on popularity of the remaining packages (\pkg{AMORE}, \pkg{FCNN4R}, \pkg{monmlp}, \pkg{qrnn}).  

A primary objective of the package is to address the myth that supervised neural networks are `black boxes' that provide no information about underlying relationships between variables \citep{Paruelo97,Olden02}.  Although neural networks are considered complex relative to more conventional approaches, the theoretical foundation has many parallels with traditional statistical techniques that can allow insight into causation \citep{Cheng94}.  Moreover, the model fitting process minimizes error using a standard objective function such that conventional techniques to evaluate model sensitivity or performance (e.g., cross-validation) can be used with neural networks.  As such, functions in \pkg{NeuralNetTools} can facilitate the selection of the optimal network architecture or can be used for post-hoc assessment. 

A more compelling issue is when and how to apply neural networks given alternative methods of analysis.  The popularity of the \ac{mlp} neural network is partially the cause of generalizations and misperceptions of their benefits as modelling tools \citep{Burke97}. Perhaps an overstatement, the neural component is commonly advertised as a mathematical representation of the network of synaptic impulses in the human brain.  Additionally, several examples have shown that the \ac{mlp} network may provide comparable predictive performance as similar statistical methods \citep{Feng02,Razi05,Beck14a}.  A neural network should be considered a tool in the larger toolbox of data-intensive methods that should be used after examination of the tradeoffs between techniques, with particular emphasis on the specific needs of a dataset or research question.  Considerations for choosing a method may include power given the sample size, expected linear or non-linear interactions between variables, distributional forms of the response, and other relevant considerations of exploratory data analysis \citep{Zuur10}.  \pkg{NeuralNetTools} provides analysis tools that can inform a more balanced assessment given alternative and complementary methods for data exploration.

\section[Acknowledgments]{Acknowledgments}
 
I thank Bruce Vondracek, Sanford Weisberg, and Bruce Wilson of the University of Minnesota for general guidance during the development of this package.  Thanks to Sehan Lee and Marc Weber for reviewing an earlier draft.  Contributions and suggestions from online users have also greatly improved the utility of the package.  Funding for this project was supported in part by an Interdisciplinary Doctoral Fellowship provided by the Graduate School at the University of Minnesota to M. Beck.  

% note that ref titles need to be in title-case 
% \bibliographystyle{jss}
% \bibliography{refs}
% this file is bbl edited by hand after running bibtex
\input{CopyOfnnt_manu.bbl}

\end{document}
