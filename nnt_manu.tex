\documentclass[article,shortnames]{jss}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{acronym}
\usepackage{cleveref}
\acrodef{mlp}[MLP]{multilayer perceptron}
\acrodef{nid}[NID]{neural interpretation diagram}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Marcus W. Beck\\Oak Ridge Institute for Science and Education\\US Environmental Protection Agency}
\title{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Marcus W. Beck} %% comma-separated
\Plaintitle{NeuralNetTools: Visualization and Analysis Tools for Neural Networks} %% without formatting
\Shorttitle{\pkg{NeuralNetTools}: Visualization and Analysis Tools for Neural Networks} 

%% an abstract and keywords
\Abstract{
Functions within this package can be used for the interpretation of neural network models created in \proglang{R}, including functions to plot a neural network interpretation diagram, evaluation of variable importance, and a sensitivity analysis of input variables.
}
\Keywords{neural networks, plotnet, sensitivity, variable importance, \proglang{R}}
\Plainkeywords{neural networks, plotnet, sensitivity, variable importance, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Marcus W. Beck\\
  Oak Ridge Institude for Science and Education\\
  US Environmental Protection Agency\\
  National Health and Environmental Effects Research Laboratory\\
  Gulf Ecology Division, 1 Sabine Island Drive\\
  Gulf Breeze, Florida, 32561, USA\\
  E-mail: \email{beck.marcus@epa.gov}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

% knitr options

% get online bib file


%% need no \usepackage{Sweave.sty}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

Data science is a relatively new paradigm of analysis that focuses on the synthesis of unstructured information from multiple sources to identify patterns or trends `born from the data' \citep{Kelling09}.  A central theme is the focus on data exploration and prediction as compared to hypothesis-testing using domain-specific methods for scientific exploration \citep{Kell03}.  Demand for quantitative toolsets to address challenges in data-rich environments has increased drastically with the advancement of techniques for rapid acquisition of data. Fields of research characterized by high-throughput data have a strong foundation in computationally-intensive methods of analysis (e.g., \citet{Saeys07}).  Morever, disciplines that have historically been limited by data quantity, such as ecological studies across broad temporal and spatial scales, have also realized the importance of data intensive approaches given the increasing use of novel techniques to acquire information (e.g., \citet{Swanson15}).  Regardless of the discipline, quantitative methods that explicitly focus on inductive reasoning can serve a complementary role to conventional, hypothesis-driven approaches to scientific discovery \citep{Kell03}.  

Statistical methods that have been used to support data exploration for inductive analysis are numerous \citep{Jain00}.  A common theme among these methods is the use of machine-learning algorithms where the primary objective is to identify emergent patterns in the data with minimal human intervention.  Neural networks, in particular, are designed to mimic the neuronal structure of the human brain by `learning' inherent data structures through adaptive algorithms \citep{Rumelhart86,Ripley96}.  Although the conceptual model was introduced several decades ago \citep{McCulloch43}, neural networks have had a central role in data intensive science.  The most popular form of neural network is the feed-forward \ac{mlp} trained using the backpropagation algorithm \citep{Rumelhart86}.  This model is typically used to predict the response of one or more variables given one to many explanatory variables.  The hallmark feature of the \ac{mlp} is the characterization of relationships using an arbitrary number of parameters (i.e., the hidden layer) that are chosen through an iterative training process with the backpropation algorithm.  Conceptually, the \ac{mlp} is nothing more than a hyper-parameterized non-linear model that can fit a smooth function to any dataset with almost non-existent residual error \citep{Hornik91}.

An arbitrarily large number of parameters to fit a neural network provides obvious predictive advantages, but conversely complicates the extraction of critical model information.  Information such as variable importance or model sensitivity are necessary aspects of exploratory data analysis that are not easily obtained from a neural network. As such, a common criticism is that neural networks are `black-boxes' that offer minimal insight into relationships among variables \citep[e.g.,][]{Paruelo97}.  \citet{Olden02} provide a rebuttal to this concern by describing methods to extract information from neural networks, most of which were previously available but not commonly used.  For example, \citet{Olden02} describe  the \ac{nid} for plotting \citep{Ozesmi99}, the Garson algorithm for variable importance \citep{Garson91}, and the Profile method for sensitivity analysis \citep{Lek96}.  These quantitative tools `illuminate the black box' by disaggregating the network parameters to characterize relationships between variables that are described by the model.  In essence, \ac{mlp} neural networks were developed for prediction but methods described in \citep{Olden02} leverage these models to describe data signals.  Increasing the accesibility of these diagnostic tools will have value for exploratory analysis in data science.

This article describes the \pkg{NeuralNetTools} package for \proglang{R} that was developed to improve the breadth and quality of information obtained from the \ac{mlp} neural network.  Functions provided by the package are those previously described in \citep{Olden02} but have not been available in an open-source programming environment.  The reach of the package is all-inclusive such that generic functions were developed using S3 methods for all neural network object classes available in \proglang{R}.  The objecives of this article are to 1) provide an overview of the statistical foundation the \ac{mlp} network, 2) briefly describe similarities and differences between existing neural network packages in \proglang{R}, and 3) describe the theory and application of the primary functions in the \pkg{NeuralNetTools} package.  The package is currently available on CRAN, whereas the development version is maintained as a GitHub repository.  

\section[Theoretical foundation]{Theoretical foundation and existing R packages}

An intriguing characteristic of neural networks is the relaxation of assumptions for the distributional characteristics of the response variables.  Unlike conventional approaches, neural networks have been popularized by their ability to model response variables with arbitrary distributions and can describe relationships in datasets with noisy or imprecise information.  The typical \ac{mlp} network is composed of multiple layers that define the transfer of information between input and response layers.  Information travels in one direction where a set of values for one to many variables in the input layer propagates through one or more hidden layers to the resulting layer of the response variables. `Hidden' layers between the input and response layers are key components of a neural network that mediate the transfer of information.  Just as the input and response layers are composed of variables or `nodes', each hidden layer is composed of nodes with weighted connections that define the strength of information flow between layers.  `Bias' layers connected to hidden and response layers may also be used that are analagous to intercept terms in a standard regression model.

Training a neural network model requires identifying the `optimal' weights that define the connections between the model layers.  The optimal weights are those that minimize prediction error for a test dataset that is independent of the training dataset.  Training is commonly achieved using the backpropagation algorithm described in detail in \citep{Rumelhart86}.  This algorithm identifies the optimal weighting scheme between layers through an iterative process where weights are gradually changed through a forward- and backward-propagation process \citep{Rumelhart86,Lek00}.  The algorithm begins by assigning an arbitrary weighting scheme to the connections in the network, followed by estimating the output in the response variable through the forward-propagation of information through the network, and finally calculating the difference between the predicted and actual value of the response.  The weights are then changed through a back-propagation step that begins by changing weights in the output layer and then the remaining hidden layers.  The process is repeated until the chosen error function is minimized.  Formulaically, a generic \ac{mlp} neural network can be represented as \citep{Ripley96}:
\begin{equation}
y_k = f_k \left(\sum\limits_{j=1}^k w_{jk}f_j \left( \sum\limits_{i=1}^j w_{ij}x_i\right) \right)
\end{equation}
where the estimated value of the response variable $y_k$ is a summation of the product between multiple input variables $x$ and multiple hidden nodes $j$, as mediated by the respective weights $w$ and activation functions $f_j$ and $f_k$ for each hidden and output node as the information progresses through the network.



Methods in \pkg{NeuralNetTools} were written for several \proglang{R} packages that can be used to create \ac{mlp} neural networks: \pkg{neuralnet} \citep{Fritsch12}, \pkg{nnet} \citep{Venables02}, and \pkg{RSNNS} \citep{Bergmeir12}. Limited methods were also developed for neural network objects created with the \code{train} function from the \pkg{caret} package \citep{Kuhn15}.  Additional \proglang{R} packages that can create \ac{mlp} neural networks include \pkg{AMORE} that implements the ``TAO-robust back-propagation algorithm'' for model fitting \citep{Castejon14}, \pkg{FCNN4R} as an \proglang{R} interface to the \pkg{FCNN} \proglang{C++} library \citep{Klima15}, \pkg{monmlp} for networks with partial monotonicity constraints \citep{Cannon15}, and \pkg{qrnn} for quantile regression neural networks \citep{Cannon11}.  At the time of writing, the CRAN download logs \citep{Csardi15} showed that the \proglang{R} packages with methods in \pkg{NeuralNetTools} included 93\% of all downloads for the available \ac{mlp} packages, with \pkg{nnet} accounting for over 78\%.  As such, methods have not been included in \pkg{NeuralNetTools} for the remaining packages, although further development of \pkg{NeuralNetTools} could include additional methods based on popularity.  Methods are currently available for  \code{mlp} (\pkg{RSNNS}), \code{nn} (\pkg{neuralnet}), \code{nnet} (\pkg{nnet}), and \code{train} (\pkg{caret}, only if the object also inherits from the \code{nnet} class) objects.  Additional \code{default} or \code{numeric} methods are available for some of the generic functions.

\section[Package structure]{Package structure}

The stable release of \pkg{NeurelNetTools} can be installed from CRAN and the development version can be installed from GitHub:

\begin{kframe}
\begin{alltt}
\hlcom{# install from CRAN}
\hlkwd{install.packages}\hlstd{(}\hlstr{'NeuralNetTools'}\hlstd{)}
\hlkwd{library}\hlstd{(NeuralNetTools)}

\hlcom{# install from GitHub}
\hlkwd{install.package}\hlstd{(}\hlstr{'devtools'}\hlstd{)}
\hlkwd{library}\hlstd{(devtools)}
\hlkwd{install_github}\hlstd{(}\hlstr{'fawda123/NeuralNetTools'}\hlstd{)}
\hlkwd{library}\hlstd{(NeuralNetTools)}
\end{alltt}
\end{kframe}

\pkg{NeuralNetTools} includes four main functions that were developed following techniques described in \citet{Olden02} and references therein.  These functions include \code{plotnet} to plot a neural network interpretation diagram, \code{garson} and \code{olden} functions to evaluate variable importance, and \code{lekprofile} for a sensitivity analysis of neural network response to input variables.  Most of the functions require the extraction of model weights in a common format for each of the neural network object classes in \proglang{R}.  The \code{neuralweights} function can be used to retrieve model weights for any of the model classes describe above.  A two-element \code{list} is returned with the first element describing the structure of the network and the second element as a named list of weight values for the input model.  The function is used internally within the main package functions but may be useful for comparing networks of different object classes.

A sample dataset is also included with \pkg{NeuralNetTools}.  The \code{neuraldat} dataset is a simple \code{data.frame} with 300 rows of observations and columns for two response variables (\code{Y1} and \code{Y2}) and three input variables (\code{X1}, \code{X2}, and \code{X3}.  The input variables are random observations from a standard normal distribution and the response variables are linear combinations of the input variables with additional random components.  The response variables are also standardized from zero to one.  A common approach for data preprocessing prior to creating a nueral network is to normalize the input variables and to standardize the response variables \citep{Lek00,Olden02}.  Novel datasets can be preprocessed to this common format using the \code{scale} function from \pkg{base} and the \code{rescale} function from \pkg{scales} for the input and response variables, respectively.  The examples below use three models created from the \code{neuraldat} dataset and include \code{mlp} (\pkg{RSNNS}), \code{nn} (\pkg{RSNNS}), and \code{nnet} (\pkg{nnet}) objects (note the syntax differences).


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# set random seed for initial weights for training}
\hlkwd{set.seed}\hlstd{(}\hlnum{123}\hlstd{)}

\hlcom{# mlp object, RSNNS package}
\hlkwd{library}\hlstd{(RSNNS)}
\hlstd{x} \hlkwb{<-} \hlstd{neuraldat[,} \hlkwd{c}\hlstd{(}\hlstr{'X1'}\hlstd{,} \hlstr{'X2'}\hlstd{,} \hlstr{'X3'}\hlstd{)]}
\hlstd{y} \hlkwb{<-} \hlstd{neuraldat[,} \hlstr{'Y1'}\hlstd{]}
\hlstd{mod1} \hlkwb{<-} \hlkwd{mlp}\hlstd{(x, y,} \hlkwc{size} \hlstd{=} \hlnum{5}\hlstd{)}

\hlcom{# nn object, neuralnet package}
\hlkwd{library}\hlstd{(neuralnet)}
\hlstd{mod2} \hlkwb{<-} \hlkwd{neuralnet}\hlstd{(Y1} \hlopt{~} \hlstd{X1} \hlopt{+} \hlstd{X2} \hlopt{+} \hlstd{X3,} \hlkwc{data} \hlstd{= neuraldat,} \hlkwc{hidden} \hlstd{=} \hlnum{5}\hlstd{)}

\hlcom{#nnet object, nnet package}
\hlkwd{library}\hlstd{(nnet)}
\hlstd{mod3} \hlkwb{<-} \hlkwd{nnet}\hlstd{(Y1} \hlopt{~} \hlstd{X1} \hlopt{+} \hlstd{X2} \hlopt{+} \hlstd{X3,} \hlkwc{data} \hlstd{= neuraldat,} \hlkwc{size} \hlstd{=} \hlnum{5}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsection{Visualizing neural networks}

The number of existing functions in \pkg{R} to view neural networks is minimal.  Such tools have practical use for visualizing network architecture and connections between layers that mediate variable importance. To our knowledge, only the \pkg{neuralnet} and \pkg{FCNN4R} packages provide plotting methods for \ac{mlp} networks in R.  Although useful for viewing the basic structure, the output is minimal and does not include extensive options for customization.

Tbe \code{plotnet} function in \pkg{NeuralNetTools} plots a network as a \acl{nid} \citep{Ozesmi99} and includes several options to customize plot aesthetics. A \ac{nid} is a modification to the standard conceptual illustration of the \ac{mlp} network that allows the user to view the thickness and color of the weight connections based on mangnitude and sign, respectively.  The default settings plot positive weights between layers as black lines and negative weights as grey lines. Line thickness is in proportion to absolute magnitude of each weight (\cref{fig:plotnet}).

\begin{kframe}
\begin{alltt}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{))}
\hlkwd{plotnet}\hlstd{(mod3,} \hlkwc{nid} \hlstd{= F,} \hlkwc{circle_col} \hlstd{=} \hlstr{'grey'}\hlstd{,} \hlkwc{bord_col} \hlstd{=} \hlstr{'grey'}\hlstd{)}
\hlkwd{plotnet}\hlstd{(mod3)}
\end{alltt}
\end{kframe}\begin{figure}[!ht]

{\centering \includegraphics[width=7.5cm,height=7.5cm]{figs/plotnet-1} 
\includegraphics[width=7.5cm,height=7.5cm]{figs/plotnet-2} 

}

\caption[Examples from the \code{plotnet} function showing neural networks as a standard graphic (left) and using the \acl{nid} (right)]{Examples from the \code{plotnet} function showing neural networks as a standard graphic (left) and using the \acl{nid} (right).  Labels outside of the nodes represent variable names and labels within the nodes indicate the layer and node (I: input, H: hidden, O: output, B: bias).  Default options are changed for the example.}\label{fig:plotnet}
\end{figure}



A primary and skip layer network can also be plotted for \code{nnet} models with a skip layer connection. The default is to plot the primary network, whereas the skip layer network can be viewed with \code{skip = TRUE}. If \code{nid = TRUE}, the line widths for both the primary and skip layer plots are relative to all weights. Viewing both plots is recommended to see which network has larger relative weights. Plotting a network with only a skip layer (i.e., no hidden layer, \code{size = 0}) will include bias connections to the output layer, whereas these are not included in the plot of the skip layer if size is greater than zero.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# create a model with a skip layer}
\hlstd{modskip} \hlkwb{<-} \hlkwd{nnet}\hlstd{(Y1} \hlopt{~} \hlstd{X1} \hlopt{+} \hlstd{X2} \hlopt{+} \hlstd{X3,} \hlkwc{data} \hlstd{= neuraldat,} \hlkwc{size} \hlstd{=} \hlnum{5}\hlstd{,} \hlkwc{skip} \hlstd{=} \hlnum{TRUE}\hlstd{)}

\hlcom{# plot}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{))}
\hlkwd{plotnet}\hlstd{(modskip,} \hlkwc{skip} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlkwd{plotnet}\hlstd{(modskip)}
\end{alltt}
\end{kframe}\begin{figure}[!ht]

{\centering \includegraphics[width=7.5cm,height=7.5cm]{figs/plotnet_skip-1} 
\includegraphics[width=7.5cm,height=7.5cm]{figs/plotnet_skip-2} 

}

\caption{Examples from the \code{plotnet} function showing a neural network with a separate skip layer between the input and output layers.  The skip layer (left) and primary neural network (right) can be viewed separately with \code{plotnet}.}\label{fig:plotnet_skip}
\end{figure}


\end{knitrout}

The \pkg{RSNNS} package provides several algorithms that can be used to prune connections or nodes in a neural network.  This approach can remove connection weights between layers or input nodes that do not contribute to the predictive performance of the network.  Pruning has the benefit of a potential increase in power by increasing the degrees of freedom or the identification of input nodes with low importance.  Algorithms in \pkg{RSNNS} for weight pruning include magnitude based pruning, Optimal Brain Damage, and Optimal Brain Surgeon, whereas algorithms for node pruning include skeletonization and the non-contributing units method \citep{Zell98}.  The \code{plotnet} function can be used to plot a pruned neural network, with options to omit or display the pruned connections (\cref{fig:plotprune}).  

\begin{kframe}
\begin{alltt}
\hlcom{# pruned model using RSNNS}
\hlstd{pruneFuncParams} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{max_pr_error_increase} \hlstd{=} \hlnum{10.0}\hlstd{,} \hlkwc{pr_accepted_error} \hlstd{=} \hlnum{1.0}\hlstd{,}
  \hlkwc{no_of_pr_retrain_cycles} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{min_error_to_stop} \hlstd{=} \hlnum{0.01}\hlstd{,}
  \hlkwc{init_matrix_value} \hlstd{=} \hlnum{1e-6}\hlstd{,} \hlkwc{input_pruning} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{hidden_pruning} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{mod} \hlkwb{<-} \hlkwd{mlp}\hlstd{(x, y,} \hlkwc{size} \hlstd{=} \hlnum{5}\hlstd{,} \hlkwc{pruneFunc} \hlstd{=} \hlstr{"OptimalBrainSurgeon"}\hlstd{,}
 \hlkwc{pruneFuncParams} \hlstd{= pruneFuncParams)}

\hlcom{# plot, pruned connections are omitted (default) or included with prune_col}
\hlkwd{par}\hlstd{(}\hlkwc{mar} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{))}
\hlkwd{plotnet}\hlstd{(mod,} \hlkwc{rel_rsc} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{8}\hlstd{))}
\hlkwd{plotnet}\hlstd{(mod,} \hlkwc{prune_col} \hlstd{=} \hlstr{'lightblue'}\hlstd{,} \hlkwc{rel_rsc} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{8}\hlstd{))}
\end{alltt}
\end{kframe}\begin{figure}[!ht]

{\centering \includegraphics[width=0.48\textwidth]{figs/plotprune-1} 
\includegraphics[width=0.48\textwidth]{figs/plotprune-2} 

}

\caption{A pruned neural network using the ``Optimal Brain Surgeon'' algorithm described in \citet{Zell98}.  The default plotting behavior of \code{plotnet} is to omit pruned connections (left), whereas they can be viewed as dashed lines by including the \code{prune\_col} argument (right).}\label{fig:plotprune}
\end{figure}



\subsection{Evaluating variable importance}

The primary benefit of visualizing a \ac{nid} with \code{plotnet} is the ability to evaluate network architecture and the variation in weighted connections between the layers.  Although useful as a general tool, the \ac{nid} is impractical for evaluating most networks given the amount of weighted connections.  Alternative methods to quantitatively describe a neural network deconstruct the model weights to determine variable importance, whereas similar information can only be qualitatively inferred from \code{plotnet}.  Two algorithms for evaluating variable importance are available in \pkg{NeuralNetTools}: Garson's algorithm for relative importance \citep{Garson91,Goh95} and Olden's conection weights algorithm \citep{Olden04}.

Garson's algorithm was originally described by \citet{Garson91} and further modified by \citet{Goh95}.  The \code{garson} function is an implementation of the method described in the appendix of \citet{Goh95}.  This method identifies the relative importance of each variable as an absolute magnitude by deconstructing all weighted connections between the layers in the network. For each input node, all weights connecting an input through the hidden layer to the response variable are identified to return a list of all weights specific to each input variable. A summed product of the connections for each input node are then scaled relative to all other inputs. A value for each input node indicates relative importance as the absolute magnitude from zero to one. The method is limited in that the direction of the response cannot be determined and only neural networks with one hidden layer and one output node can be evaluated.

The \code{olden} function is a more flexible approach to evaluate variable importance using the connection weights algorithm described in \citet{Olden04}. This method calculates importance as the product of the raw input-hidden and hidden-output connection weights between each input and output node and sums the product across all hidden nodes. An advantage is the relative contributions of each connection weight are maintained in both magnitude and sign, as compared to Garson's algorithm which only considers absolute magnitude. For example, connection weights that change sign (e.g., positive to negative) between the input-hidden to hidden-output layers would have a cancelling effect whereas Garson's algorithm may provide misleading results based on the absolute magnitude. An additional advantage is that Olden's algorithm is capable of evaluating neural networks with multiple hidden layers and response variables. The importance values assigned to each variable are also in units based on the summed product of the connection weights, whereas \code{garson} returns importance scaled from 0--1.

Both functions have similar implementations and require only a model object as input.  The default output is a \pkg{ggplot2} bar plot \citep[i.e., \code{geom\_bar},][]{Wickham09} that shows the relative importance of each input variable in the model.  The plot aesthetics are based on internal code from the function but can be changed using conventional syntax for \pkg{ggplot2}.  The importance values can also be returned as a \code{data.frame} if \code{bar\_plot = FALSE}.  

\begin{kframe}
\begin{alltt}
\hlcom{# garson and olden functions for each model}
\hlkwd{garson}\hlstd{(mod1)}
\hlkwd{olden}\hlstd{(mod1)}
\hlkwd{garson}\hlstd{(mod2)}
\hlkwd{olden}\hlstd{(mod2)}
\hlkwd{garson}\hlstd{(mod3)}
\hlkwd{olden}\hlstd{(mod3)}
\end{alltt}
\end{kframe}\begin{figure}[!ht]

{\centering \includegraphics[width=0.45\textwidth]{figs/plotimp-1} 
\includegraphics[width=0.45\textwidth]{figs/plotimp-2} 
\includegraphics[width=0.45\textwidth]{figs/plotimp-3} 
\includegraphics[width=0.45\textwidth]{figs/plotimp-4} 
\includegraphics[width=0.45\textwidth]{figs/plotimp-5} 
\includegraphics[width=0.45\textwidth]{figs/plotimp-6} 

}

\caption[Variable importance]{Variable importance}\label{fig:plotimp}
\end{figure}



\subsection{Sensitivity analysis}

The Lek profile method is described briefly in \citet{Lek96} and in more detail in \citet{Gevrey03}. The profile method is fairly generic and can be extended to any statistical model in R with a predict method. However, it is one of few methods used to evaluate sensitivity in neural networks.

The profile method can be used to evaluate the effect of explanatory variables by returning a plot of the predicted response across the range of values for each separate variable. The original profile method evaluated the effects of each variable while holding the remaining expalanatory variables at different quantiles (e.g., minimum, 20th percentile, maximum). This is implemented in in the function by creating a matrix of values for explanatory variables where the number of rows is the number of observations and the number of columns is the number of explanatory variables. All explanatory variables are held at their mean (or other constant value) while the variable of interest is sequenced from its minimum to maximum value across the range of observations. This matrix (or data frame) is then used to predict values of the response variable from a fitted model object. This is repeated for each explanatory variable to obtain all response curves. Values passed to \code{split_vals} must range from zero to one to define the quantiles for holding unevaluated explanatory variables.

An alternative implementation of the profile method is to group the unevaluated explanatory variables using groupings defined by the statistical properties of the data. Covariance among predictors may present unlikely scenarios if holding all unevaluated variables at the same level. To address this issue, the function provides an option to hold unevalutaed variable at mean values defined by natural clusters in the data. kmeans clustering is used on the input data.frame of explanatory variables if the argument passed to \code{split_vals} is an integer value greater than one. The centers of the clusters are then used as constant values for the unevaluated variables. An arbitrary grouping scheme can also be passed to \code{split_vals} as a data.frame where the user can specify exact values for holding each value constant (see the examples).  Examples in \citet{Beck14a} show this...

For all plots, the legend with the 'splits' label indicates the colors that correspond to each group. The groups describe the values at which unevaluated explanatory variables were held constant, either as specific quantiles, group assignments based on clustering, or in the arbitrary grouping defined by the user. The constant values of each explanatory variable for each split can be viewed as a barplot by using \code{split_show = TRUE}.

Note that there is no predict method for neuralnet objects from the nn package. The lekprofile method for nn objects uses the nnet package to recreate the input model, which is then used for the sensitivity predictions. This approach only works for networks with one hidden layer.

\section[Future development]{Future development}


\section[Conclusions]{Conclusions}
Issues with different indications of variable importance as a model is refit.

A cautionary note about the `optimal network' and reproducibility of results.  Note that most literature sources suggest variable standardization prior to using a model yet none of the existing packages in R provide this functionality as a default option (verify).    

\section[Acknowledgments]{Acknowledgments}

% note that ref titles need to be in title-case 
\bibliographystyle{jss}
\bibliography{refs}

\end{document}
